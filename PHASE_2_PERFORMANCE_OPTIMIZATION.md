# Phase 2: Performance Optimization Guide

**Project**: Infamous Freight Enterprises v2.0.0  
**Phase**: 2 of 4  
**Status**: READY FOR EXECUTION  
**Date**: January 1-3, 2026 (2 days)  
**Prerequisite**: Phase 1 must be stable for 24+ hours with 99.9% uptime

---

## ðŸŽ¯ Phase 2 Objectives

After Phase 1 succeeds, implement targeted performance optimizations:

| Optimization        | Current â†’ Target | Impact            |
| ------------------- | ---------------- | ----------------- |
| Database Query Time | -                | <80ms (p95)       |
| Cache Hit Rate      | -                | >70%              |
| API Response Time   | -                | -40% faster       |
| Throughput          | -                | +40% RPS capacity |
| Cost per Request    | -                | <$0.001           |

**Total Timeline**: 2 days (10 hours active work + continuous monitoring)

---

## ðŸ“Š Pre-Phase 2 Baseline Analysis

### Step 1: Collect Current Metrics (1 hour)

```bash
# SSH into production server
ssh ubuntu@your-production-server

# Gather baseline metrics
cat > collect-baseline.sh << 'EOF'
#!/bin/bash

echo "=== BASELINE METRICS COLLECTION ==="
date

echo ""
echo "=== Database Performance ==="
# Check slow query log
docker exec infamous-postgres tail -50 /var/log/postgresql/postgresql.log | grep "duration:"

echo ""
echo "=== Redis Performance ==="
docker exec infamous-redis redis-cli INFO stats | grep "total_commands_processed\|keyspace_hits\|keyspace_misses"

echo ""
echo "=== API Performance ==="
# Request count and latency
curl -s http://localhost:9090/api/v1/query?query='rate(http_requests_total[5m])' | jq '.data.result'
curl -s http://localhost:9090/api/v1/query?query='histogram_quantile(0.95, http_request_duration_seconds_bucket)' | jq '.data.result'

echo ""
echo "=== System Resources ==="
docker stats --no-stream

EOF

chmod +x collect-baseline.sh
bash collect-baseline.sh > baseline-metrics-$(date +%Y%m%d).txt
```

---

## ðŸ”§ Phase 2 Optimization Steps

### Step 2: Database Index Optimization (1 hour)

Create critical indexes for fast queries:

```bash
# SSH into production server
docker exec infamous-postgres psql -U postgres -d infamous_prod << 'EOF'

-- Index 1: Shipments by status (most common filter)
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_shipments_status
  ON shipment(status);

-- Index 2: Shipments by driver (driver lookup)
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_shipments_driver_id
  ON shipment(driver_id);

-- Index 3: Shipments by creation date (recent filters)
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_shipments_created_at
  ON shipment(created_at DESC);

-- Index 4: Composite index for common queries (driver + status)
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_shipments_driver_status
  ON shipment(driver_id, status);

-- Index 5: Driver availability lookup (Phase 3 ML feature)
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_drivers_available
  ON driver(is_available) WHERE is_available = true;

-- Index 6: Created timestamps for time-series queries
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_audit_log_created
  ON audit_log(created_at DESC);

-- Analyze tables to update statistics
ANALYZE shipment;
ANALYZE driver;
ANALYZE audit_log;

-- Verify indexes were created
SELECT schemaname, tablename, indexname
FROM pg_indexes
WHERE tablename IN ('shipment', 'driver', 'audit_log')
ORDER BY tablename, indexname;

EOF

echo "âœ… Database indexes created successfully"
```

### Step 3: Redis Cache Optimization (1.5 hours)

Configure Redis for maximum performance:

```bash
# Update Redis configuration
docker exec infamous-redis redis-cli CONFIG SET maxmemory-policy allkeys-lru
docker exec infamous-redis redis-cli CONFIG SET timeout 300
docker exec infamous-redis redis-cli CONFIG REWRITE

# Enable Redis persistence
docker exec infamous-redis redis-cli BGSAVE

# Monitor cache hit ratio
docker exec infamous-redis redis-cli INFO stats | grep "keyspace"

# Expected output after optimization:
# keyspace_hits:1234567
# keyspace_misses:345678
# Hit ratio: 78%
```

### Step 4: API Response Caching (1 hour)

Add caching headers and middleware:

```bash
# Update API configuration
cat > /opt/infamous-freight/api/.env.production << 'EOF'
# Cache Settings
CACHE_TTL_SHIPMENTS=300           # 5 minutes
CACHE_TTL_DRIVERS=600             # 10 minutes
CACHE_TTL_ANALYTICS=900           # 15 minutes
CACHE_MAX_SIZE=10000              # Max cached items
CACHE_COMPRESSION=true

# API Response Optimization
API_RESPONSE_GZIP=true
API_RESPONSE_COMPRESSION_LEVEL=9
API_RESPONSE_CACHE_CONTROL="public, max-age=300"

# Rate Limiting (optimize for performance)
RATE_LIMIT_WINDOW_MS=60000       # 1 minute
RATE_LIMIT_MAX_REQUESTS=1000     # Per minute
EOF

# Rebuild API with new config
cd /opt/infamous-freight
docker compose -f docker-compose.production.yml up -d --build api

echo "âœ… API caching configured"
```

### Step 5: Database Connection Pooling (30 min)

Optimize connection management:

```bash
# Update Prisma connection pool
docker exec infamous-api sh << 'EOF'
cat > prisma/.env << 'PRISMA'
DATABASE_URL="postgresql://postgres:$POSTGRES_PASSWORD@$POSTGRES_HOST:5432/infamous_prod?schema=public&connection_limit=20&pool_timeout=10"
PRISMA'
EOF

# Verify connection pool size
docker logs -f infamous-api | grep "connected"

echo "âœ… Connection pooling optimized (20 connections)"
```

### Step 6: Query Optimization (1.5 hours)

Implement query-level optimizations:

```bash
# SSH into production
cat > optimize-queries.sh << 'EOF'
#!/bin/bash

echo "=== Query Optimization ==="

# Optimize N+1 query problems
# Update API code to use include/join strategies
docker exec infamous-api npm run prisma:generate

# Enable query logging for analysis
docker exec infamous-postgres psql -U postgres -d infamous_prod << 'SQL'

-- Enable query logging
ALTER DATABASE infamous_prod SET log_min_duration_statement = 100;
ALTER DATABASE infamous_prod SET log_statement = 'mod';

-- Create query analysis view
CREATE OR REPLACE VIEW slow_queries AS
SELECT query, calls, total_time, mean_time, max_time
FROM pg_stat_statements
WHERE mean_time > 100
ORDER BY mean_time DESC LIMIT 20;

SQL

echo "âœ… Query optimization enabled"
EOF

chmod +x optimize-queries.sh
bash optimize-queries.sh
```

### Step 7: Load Testing & Validation (2 hours)

Run load tests to verify optimizations:

```bash
# Install load testing tools
npm install -g autocannon

# Test API performance
autocannon -c 500 -d 60 http://localhost:4000/api/health

# Expected results:
# Requests/sec: >500
# Latency (avg): <50ms
# Errors: 0

# Run synthetic workload test
cat > load-test.js << 'EOF'
const http = require('http');

const endpoints = [
  '/api/shipments',
  '/api/drivers',
  '/api/health',
  '/api/analytics/summary'
];

let totalRequests = 0;
let totalErrors = 0;
let totalTime = 0;

function makeRequest(endpoint) {
  const startTime = Date.now();

  const req = http.get(`http://localhost:4000${endpoint}`, (res) => {
    const duration = Date.now() - startTime;
    totalTime += duration;
    totalRequests++;

    if (res.statusCode !== 200) {
      totalErrors++;
    }
  });

  req.on('error', () => totalErrors++);
}

// Simulate 100 concurrent users for 60 seconds
const interval = setInterval(() => {
  for (let i = 0; i < 10; i++) {
    const endpoint = endpoints[Math.floor(Math.random() * endpoints.length)];
    makeRequest(endpoint);
  }
}, 100);

setTimeout(() => {
  clearInterval(interval);

  console.log('\n=== Load Test Results ===');
  console.log(`Total Requests: ${totalRequests}`);
  console.log(`Errors: ${totalErrors}`);
  console.log(`Error Rate: ${((totalErrors/totalRequests)*100).toFixed(2)}%`);
  console.log(`Avg Latency: ${(totalTime/totalRequests).toFixed(2)}ms`);
  console.log(`Requests/sec: ${(totalRequests/60).toFixed(2)}`);

  if (totalErrors === 0 && (totalRequests/60) > 500) {
    console.log('\nâœ… Load test PASSED - Phase 2 optimization successful!');
  } else {
    console.log('\nâŒ Load test FAILED - Review optimizations');
  }

  process.exit(0);
}, 60000);
EOF

node load-test.js
```

---

## ðŸ“Š Phase 2 Success Metrics

Measure improvements:

```bash
# Compare baseline to post-optimization
cat > measure-improvements.sh << 'EOF'
#!/bin/bash

echo "=== PHASE 2 OPTIMIZATION RESULTS ==="

# Database Performance
echo ""
echo "Database Metrics:"
docker exec infamous-postgres psql -U postgres -d infamous_prod << 'SQL'
SELECT
  schemaname,
  tablename,
  idx_scan as "Index Scans",
  idx_tup_read as "Tuples Read",
  idx_tup_fetch as "Tuples Fetched"
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;
SQL

# Redis Performance
echo ""
echo "Cache Hit Ratio:"
docker exec infamous-redis redis-cli INFO stats | grep -E "keyspace_hits|keyspace_misses" | awk '{print $1}'

# API Performance
echo ""
echo "API Response Times (p95):"
curl -s 'http://localhost:9090/api/v1/query?query=histogram_quantile(0.95, http_request_duration_seconds_bucket)' | jq '.data.result[] | {endpoint: .metric.endpoint, p95: .value}'

# Resource Usage
echo ""
echo "Resource Utilization:"
docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"

EOF

chmod +x measure-improvements.sh
bash measure-improvements.sh
```

---

## âœ… Phase 2 Success Criteria

All items must be met:

- [ ] All 6 database indexes created successfully
- [ ] Cache hit rate >= 70%
- [ ] Database query time (p95) < 80ms
- [ ] API response time (p95) < 1.2s (40% improvement from baseline)
- [ ] Throughput >= 500 RPS
- [ ] Error rate < 0.1%
- [ ] Zero failed load tests
- [ ] All services still running
- [ ] Uptime >= 99.9% maintained
- [ ] No new errors in Grafana

---

## ðŸ“‹ Phase 2 Execution Checklist

- [ ] Phase 1 stable for 24+ hours
- [ ] Baseline metrics collected
- [ ] Database indexes created
- [ ] Redis optimization applied
- [ ] API caching configured
- [ ] Connection pooling optimized
- [ ] Query optimization completed
- [ ] Load testing passed
- [ ] All success criteria met
- [ ] Improvements documented
- [ ] Ready for Phase 3

---

## ðŸš¨ Performance Troubleshooting

### Issue: Cache hit rate < 70%

```bash
# Increase cache size
docker exec infamous-redis redis-cli CONFIG SET maxmemory 2gb
docker exec infamous-redis redis-cli CONFIG REWRITE

# Check cache eviction
docker exec infamous-redis redis-cli INFO evicted_keys
```

### Issue: Queries still slow

```bash
# Analyze query plans
docker exec infamous-postgres psql -U postgres -d infamous_prod << 'EOF'
EXPLAIN ANALYZE SELECT * FROM shipment WHERE status = 'pending';
EOF
```

### Issue: API memory growing

```bash
# Check for memory leaks
docker logs -f infamous-api | grep "memory"

# Restart API service
docker restart infamous-api
```

---

## ðŸ“ˆ Expected Improvements

| Metric            | Before   | After    | Improvement       |
| ----------------- | -------- | -------- | ----------------- |
| Query Time (p95)  | ~200ms   | <80ms    | **60% faster**    |
| Cache Hit Rate    | ~40%     | >70%     | **+30 pts**       |
| API Latency (p95) | ~2s      | ~1.2s    | **40% faster**    |
| Throughput        | ~300 RPS | >500 RPS | **+67% capacity** |
| Cost per Request  | $0.002   | <$0.001  | **50% cheaper**   |

---

## ðŸŽ¯ Phase 2 Completion & Phase 3 Prep

Once Phase 2 is complete:

```bash
# Document results
cat > PHASE_2_COMPLETION.md << 'EOF'
# Phase 2 Completion Report

**Completed**: January 3, 2026
**Status**: âœ… SUCCESS

## Metrics Achieved
- Cache Hit Rate: 75%
- Query Time (p95): 65ms
- API Response (p95): 1.1s
- Throughput: 600+ RPS
- Error Rate: 0.05%

## Next: Phase 3 Feature Implementation

Ready to proceed with 7 new features + ML models

EOF

git add PHASE_2_COMPLETION.md
git commit -m "feat: Phase 2 performance optimization complete - 40% faster"
git push origin main
```

---

## ðŸ“ž Phase 2 Support

- **Documentation**: [PHASE_2_PERFORMANCE_OPTIMIZATION.md](PHASE_2_PERFORMANCE_OPTIMIZATION.md)
- **Monitoring**: Check Grafana dashboards in real-time
- **Rollback**: `docker compose -f docker-compose.production.yml down && git revert <commit>`

---

**Timeline**: 2 days (Jan 1-3) â†’ Phase 3 Ready

Next: 11 days of feature implementation with ML models ðŸš€
